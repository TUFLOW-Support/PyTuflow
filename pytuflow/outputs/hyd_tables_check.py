import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Union

import numpy as np
import pandas as pd

from .tabular_output import TabularOutput
from .helpers.get_standard_data_type_name import get_standard_data_type_name
from .helpers.hyd_tables_cross_section_provider import HydTablesCrossSectionProvider
from .helpers.hyd_tables_channel_provider import HydTablesChannelProvider
from ..pytuflow_types import PathLike, TimeLike, FileTypeError
from pytuflow.util.logging import get_logger


logger = get_logger()


class HydTablesCheck(TabularOutput):
    """Class for reading the TUFLOW check file for 1D hydraulic tables.
    These are file that end with :code:`_1d_ta_tables_check.csv`, found in the 1D check folder.

    Parameters
    ----------
    fpath : :class:`PathLike <pytuflow.pytuflow_types.PathLike>`
        The path to the 1D hydraulic tables check file.

    Raises
    ------
    FileNotFoundError
        Raised if the check file does not exist.
    FileTypeError
        Raises :class:`pytuflow.pytuflow_types.FileTypeError` if the file does not look like a valid check file.
    EOFError
        Raised if the check file is empty or incomplete.

    Examples
    --------
    Load a 1D hydraulic tables check file:

    >>> from pytuflow.outputs import HydTablesCheck
    >>> hyd_tables = HydTablesCheck('path/to/1d_ta_tables_check.csv')
    """

    def __init__(self, fpath: PathLike) -> None:
        super().__init__(fpath)

        self.reference_time = datetime(2000, 1, 1, tzinfo=timezone.utc)
        self.units = ''
        #: Path: The path to the source output file.
        self.fpath = Path(fpath)
        #: Path: Path to the parent TCF file
        self.tcf = Path()
        #: :class:`HydTablesCrossSectionProvider <pytuflow.outputs.helpers.hyd_tables_cross_sections.HydTablesCrossSectionProvider>`: Cross-section data provider
        self.cross_sections = HydTablesCrossSectionProvider()
        #: :class:`HydTablesChannelProvider <pytuflow.outputs.helpers.hyd_tables_channel_provider.HydTablesChannelProvider>`: Channel data provider
        self.channels = HydTablesChannelProvider()
        #: pd.DataFrame: DataFrame with all the data combinations
        self.objs = pd.DataFrame()
        #: int: Number of cross-sections
        self.cross_section_count = 0
        #: int: Number of channels
        self.channel_count = 0

        if not self.fpath.exists():
            raise FileNotFoundError(f'File does not exist: {fpath}')

        if not self.looks_like_this(self.fpath):
            raise FileTypeError(f'File does not look like a {self.__class__.__name__} file: {fpath}')

        if self.looks_empty(self.fpath):
            raise EOFError(f'File is empty or incomplete: {fpath}')

        self._load()

    @staticmethod
    def looks_like_this(fpath: Path) -> bool:
        # docstring inherited
        try:
            if not re.findall(r'_1d_ta_tables_check$', fpath.stem):
                return False
            with fpath.open() as f:
                line = f.readline()
                if not re.findall(r'^"?Generated by', line):
                    return False
        except Exception as e:
            return False
        return True

    @staticmethod
    def looks_empty(fpath: Path) -> bool:
        # docstring inherited
        try:
            with fpath.open() as f:
                for _ in range(3):
                    line = f.readline()
                if not re.findall(r'^"?Section XS\d{5}', line):
                    return True
        except Exception:
            return True
        return False

    def context_filter(self, context: str) -> pd.DataFrame:
        # docstring inherited
        # split context into components
        ctx = [x.strip().lower() for x in context.split('/') if x] if context else []

        # domain - xs, processed, or channel
        ctx_ = []
        filtered_something = False
        if np.intersect1d(ctx, ['xs', 'cross-section', 'cross_section', 'cross section']).size:
            filtered_something = True
            ctx_.append('xs')
            ctx = [x for x in ctx if x not in ['xs', 'cross-section', 'cross_section', 'cross section']]
        if np.intersect1d(ctx, ['processed', 'proc']).size:
            filtered_something = True
            ctx_.append('processed')
            ctx = [x for x in ctx if x not in ['processed', 'proc']]
        if np.intersect1d(ctx, ['channel']).size:
            filtered_something = True
            ctx_.append('channel')
            ctx = [x for x in ctx if x not in  ['channel']]
        if filtered_something:
            df = self.objs[self.objs['geometry'].isin(ctx_)]
        else:
            df = self.objs.copy()

        # type
        possible_types = ['xz', 'hw', 'cs', 'bg', 'lc']
        ctx1 = [x for x in ctx if x in possible_types]
        ctx1 = [x for x in ctx1 if x in df['type'].str.lower().unique()]
        if ctx1:
            filtered_something = True
            df = df[df['type'].str.lower().isin(ctx1)]
            j = len(ctx) - 1
            for i, x in enumerate(reversed(ctx.copy())):
                if x in ctx1:
                    ctx.pop(j - i)

        # data types
        ctx1 = [get_standard_data_type_name(x) for x in ctx]
        ctx1 = [x for x in ctx1 if x in df['data_type'].unique()]
        if ctx1:
            filtered_something = True
            df = df[df['data_type'].isin(ctx1)]
            j = len(ctx) - 1
            for i, x in enumerate(reversed(ctx.copy())):
                if get_standard_data_type_name(x) in ctx1:
                    ctx.pop(j - i)

        # ids
        if ctx and not df.empty:
            df1 = df[df['id'].str.lower().isin(ctx)]
            df2 = df[df['uid'].str.lower().isin(ctx)]
            if not df1.empty and not df2.empty:
                df = pd.concat([df1, df2], axis=0)
            elif not df1.empty:
                df = df1
            elif not df2.empty:
                df = df2
            else:
                df = pd.DataFrame()
            if not df.empty:
                j = len(ctx) - 1
                for i, x in enumerate(reversed(ctx.copy())):
                    if df['id'].str.lower().isin([x.lower()]).any() or df['uid'].str.lower().isin([x.lower()]).any():
                        ctx.pop(j - i)
                if ctx and not filtered_something:
                    df = pd.DataFrame()

        return df if not df.empty else pd.DataFrame(columns=['id', 'uid', 'type', 'data_type', 'geometry'])

    def times(self, context: str = None, fmt: str = 'relative') -> list[TimeLike]:
        """HydraulicTableCheck1D results are static and will not return any times."""
        return []  # data is static - no times exist

    def ids(self, context: str = None) -> list[str]:
        """Returns all the available IDs for the given context.

        The context argument can be used to add a filter to the returned IDs. Available context objects for this
        class are:

        * :code:`None` - returns all IDs

        Process step contexts:

        * :code:`xs` - returns the IDs that have raw cross-section data
        * :code:`processed` - returns the IDs that have processed cross-section data
        * :code:`channel` - returns the IDs for the processed channels

        Type contexts:

        * :code:`[type]` - returns the IDs that match the given cross-section type (e.g. "XZ" or "HW")

        Data type contexts:

        * :code:`[data type]` - returns the IDs that match the given data type (e.g. "Storage Width")

        Combine contexts:

        * :code:`[context1]/[context2] ...`: Combine multiple contexts to filter the IDs further ('/' delim).

        Parameters
        ----------
        context : str, optional
            The context to filter the IDs by.

        Returns
        -------
        list[str]
            The available IDs.

        Examples
        --------
        Return all IDs for cross-section of type :code:`XZ`:

        >>> hyd_tables.ids('xz')
        ['1d_xs_M14_C99', '1d_xs_M14_C100', '1d_xs_M14_C101'  ...  '1d_xs_M14_C102', '1d_xs_M14_C103', '1d_xs_M14_C104']

        Return the name of the processed channels using the tabular data:

        >>> hyd_tables.ids('channel')
        ['RD_weir', 'FC01.39', 'FC01.38'  ...  FC01.37', 'FC01.36', 'FC01.34']
        """
        df = self.context_filter(context)
        return df.id.unique().tolist()

    def data_types(self, context: str = None) -> list[str]:
        """Returns all the available data types for the given context.

        The context argument can be used to add a filter to the returned IDs. Available context objects for this
        class are:

        * :code:`None` - returns all data types

        Process step contexts:

        * :code:`xs` - returns the available data types for the raw cross-sections
        * :code:`processed` - returns the data types available for the processed cross-sections
        * :code:`channel` - returns the data types available for the processed channels

        Type contexts:

        * :code:`[type]` - returns the available data types that match the given cross-section type (e.g. "XZ" or "HW")

        Data type contexts:

        * :code:`[id]` - returns the available data types for the given ID

        Combine contexts:

        * :code:`[context1]/[context2] ...`: Combine multiple contexts to filter the data types further ('/' delim).

        Parameters
        ----------
        context : str, optional
            The context to filter the data types by.

        Returns
        -------
        list[str]
            The available data types.

        Examples
        --------
        Return all data types for the processed cross-sections:

        >>> hyd_tables.data_types('processed')
        ['depth', 'width', 'effective width', 'effective area',
        'effective wetted perimeter', 'radius', 'vertex resistance factor', 'k ']

        Returns all data types for a given channel ID:

        >>> hyd_tables.data_types('FC01.39')
        ['depth', 'storage width', 'flow width', 'area', 'wetted perimeter',
        'radius', 'vertex resistance factor', 'k ']
        """
        df = self.context_filter(context)
        return df.data_type.unique().tolist()

    def section(self, locations: Union[str, list[str]], data_types: Union[str, list[str]],
                time: TimeLike = -1) -> pd.DataFrame:
        """Returns section data for the given locations and data types from the 1D tables check file.

        This method supports returning multiple locations and data types. As a consequence,
        the returned DataFrame will use pd.MultiIndex columns since the returned DataFrame is not
        guaranteed to share a common index. The column index will be made up of 2 levels: the first level
        is the location name and what part of the processing stage the data came from (e.g. xs, processed, channel).
        The second level will be the normal column names, such as :code:`elevation` and :code:`flow width`.
        The first column of any grouped level 1 index, will be the index column.

        E.g. if you are getting elevation and manning n data for cross-section :code:`XS001` the level 1 will all be
        :code:`XS001/xs` and the level 2 indexes will be :code:`distance`, :code:`elevation`, :code:`manning n`.
        The first column (:code:`distance`) should be treated as the index column for this group.

        Parameters
        ----------
        locations : str or list[str]
            The locations to return data for. This can be a single location or a list of locations.
        data_types : str or list[str]
            The data types to return. This can be a single data type or a list of data types.
        time : TimeLike, optional
            The time to return data for. This is not used for this class and can be ignored.

        Returns
        -------
        pd.DataFrame
            The section data for the given locations and data types.

        Examples
        --------
        Return the processed :code:`flow width` for channel :code:`FC01.39`:

        >>> hyd_tables.section('FC01.39', 'flow width')
           FC01.39/channel
                 elevation flow width
        0           45.500      0.002
        1           45.517      0.929
        2           45.717      3.781
        3           45.917      4.414
        4           46.117      5.069
        ...           ...         ...
        30          51.317     23.462
        31          51.517     23.655
        32          51.717     23.848
        33          51.917     24.172
        34          51.926     24.194
        """
        # figure out locations and data types
        locations, data_types = self._figure_out_loc_and_data_types(locations, data_types)
        dtypes = [get_standard_data_type_name(x) for x in data_types]

        # get more context on the inputs - e.g. what stage of processing they are from
        ctx = '/'.join(locations + data_types)
        df = self.context_filter(ctx)

        df1 = pd.DataFrame()
        for loc in locations:
            for proc_type in ['xs', 'processed', 'channel']:
                if df[(df.id == loc) & (df.geometry == proc_type)].empty:
                    continue
                if proc_type in ['xs', 'processed']:
                    loc1 = self.cross_sections.name2id(loc) if loc not in self.cross_sections.database else loc
                    if proc_type == 'xs':
                        df_ = self.cross_sections.database[loc1].df_xs
                    else:
                        df_ = self.cross_sections.database[loc1].df_proc[data_types]
                else:
                    df_ = self.channels.database[loc][data_types]

                dtypes1 = [x for x in dtypes if x in df_.columns]
                dtypes2 = [data_types[i] for i, x in enumerate(dtypes) if x in df_.columns]
                df_ = df_[dtypes1]
                df_.columns = dtypes2
                if df_ is None or df_.empty:
                    continue
                df_.reset_index(inplace=True)
                df_.columns = pd.MultiIndex.from_product([[f'{loc}/{proc_type}'], df_.columns])
                df1 = pd.concat([df1, df_], axis=1) if not df1.empty else df_

        return df1

    def time_series(self, locations: Union[str, list[str]], data_types: Union[str, list[str]],
                    time_fmt: str = 'relative') -> pd.DataFrame:
        """Not supported for HydraulicTableCheck1D results. Raises a :code:`NotImplementedError`."""
        raise NotImplementedError(f'{__class__.__name__} does not support time-series plotting.')

    def curtain(self, locations: Union[str, list[str]], data_types: Union[str, list[str]],
                time: TimeLike) -> pd.DataFrame:
        """Not supported for HydraulicTableCheck1D results. Raises a :code:`NotImplementedError`."""
        raise NotImplementedError(f'{__class__.__name__} does not support curtain plotting.')

    def profile(self, locations: Union[str, list[str]], data_types: Union[str, list[str]],
                time: TimeLike) -> pd.DataFrame:
        """Not supported for HydraulicTableCheck1D results. Raises a :code:`NotImplementedError`."""
        raise NotImplementedError(f'{__class__.__name__} does not support vertical profile plotting.')

    def _load(self):
        self.name = re.sub(r"_1d_ta_tables_check$", '', self.fpath.stem)
        with self.fpath.open() as f:
            line = f.readline()
            s = line.split('"')
            i = 1 if 'generated by' not in s[1].lower() else 3
            self.tcf = s[i].strip()
            self.tcf = Path(self.tcf)
            while not self.cross_sections.finished:
                self.cross_sections.read_next(f)
            self.cross_section_count = len(self.cross_sections.database)
            while not self.channels.finished:
                self.channels.read_next(f)
            self.channel_count = len(self.channels.database)
        self._load_objs()

    def _load_objs(self):
        def add_xs_prop(d, xs):
            d['id'].append(xs.name)
            d['uid'].append(xs.id)
            d['type'].append(xs.type)

        d = {'id': [], 'type': [], 'uid': [], 'data_type': [], 'geometry': []}

        # cross-sections
        for id_, xs in self.cross_sections.database.items():
            if xs.has_xs:
                for col in xs.df_xs.columns:
                    if col in ['point', 'message']:
                        continue
                    add_xs_prop(d, xs)
                    d['data_type'].append(get_standard_data_type_name(col))
                    d['geometry'].append('xs')
            for col in xs.df_proc.columns:
                if col in ['point', 'message']:
                    continue
                add_xs_prop(d, xs)
                d['data_type'].append(get_standard_data_type_name(col))
                d['geometry'].append('processed')

        # channels
        for id_, ch in self.channels.database.items():
            for col in ch.columns:
                if col in ['point', 'message']:
                    continue
                d['id'].append(id_)
                d['uid'].append('')
                d['type'].append('')
                d['data_type'].append(get_standard_data_type_name(col))
                d['geometry'].append('channel')

        self.objs = pd.DataFrame(d)

    def _figure_out_loc_and_data_types(self, locations: Union[str, list[str]],
                                       data_types: Union[str, list[str], None]) -> tuple[list[str], list[str]]:
        """Figure out the locations and data types to use."""
        # sort out locations and data types
        if not locations:
            locations = self.ids()
        else:
            valid_loc = self.ids()
            valid_loc_lower = [x.lower() for x in valid_loc]
            locations1 = []
            locations = [locations] if not isinstance(locations, list) else locations
            for loc in locations:
                if loc.lower() not in valid_loc_lower:
                    logger.warning(f'HydTablesCheck.section(): Location "{loc}" not found in the output - removing.')
                else:
                    locations1.append(valid_loc[valid_loc_lower.index(loc.lower())])
            locations = locations1
            if not locations:
                raise ValueError('No valid locations provided.')

        if not data_types:
            data_types = self.data_types()
        else:
            data_types = [data_types] if not isinstance(data_types, list) else data_types
            valid_types = self.data_types()
            data_types1 = []
            for dtype in data_types:
                stndname = get_standard_data_type_name(dtype)
                if stndname not in valid_types:
                    logger.warning(
                        f'HydTablesCheck.section(): Data type "{dtype}" is not a valid section data type or '
                        f'not in output - removing.'
                    )
                else:
                    data_types1.append(dtype)
            if not data_types1:
                raise ValueError('No valid data types provided.')
            data_types = data_types1

        return locations, data_types
